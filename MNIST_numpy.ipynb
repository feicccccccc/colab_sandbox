{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feicccccccc/colab_sandbox/blob/master/MNIST_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHpf1ItFCOhC",
        "colab_type": "text"
      },
      "source": [
        "# A test to use colab to perform MNIST training using pure numpy operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xryAoDHRA4lG",
        "colab_type": "text"
      },
      "source": [
        "This notebook is base on many resource:\n",
        "1. Andrew Ng DL course\n",
        "2. deeplearning online tutorial / book\n",
        "\n",
        "And implement and document in my own way  ;)\n",
        "Targeting for easy understanidng with only high school knowledge:\n",
        "\n",
        "\n",
        "1.   Basic differentiation\n",
        "2.   Basci linear algebra (basically matrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3voLr3S-Cd8K",
        "colab_type": "text"
      },
      "source": [
        "## First step: Import library including\n",
        "\n",
        "\n",
        "1.   Numpy\n",
        "2.   keras ( to import the training set and testing set)\n",
        "3.   matplotlib ( for ploting the image)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew_MAsWOh8qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4zKH7HRC0Db",
        "colab_type": "text"
      },
      "source": [
        "### Get the MNIST data set from keras default dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_7Iz0usibE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDEsA5wDC8-0",
        "colab_type": "text"
      },
      "source": [
        "### Show part of the data set in term of pixel map and there output y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWmA1MKPqVnA",
        "colab_type": "code",
        "outputId": "599dda98-5c44-455b-bbe0-294ca70e8105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "images=[x_train[0,:],x_train[1,:],x_train[2,:],x_train[3,:],x_train[4,:]]\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "columns =5\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAADgCAYAAAB1lqE5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWsrCDwAAAEBLWfgBAAAAaKn/DwpLYjOIhhxPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x720 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdzYmKfSDl3z",
        "colab_type": "code",
        "outputId": "03790355-4dea-4db4-c7e8-8c03b753f780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"First label  = \", y_train[0])\n",
        "print(\"Second Label = \", y_train[1])\n",
        "print(\"Third label  = \", y_train[2])\n",
        "print(\"Forth label  = \", y_train[3])\n",
        "print(\"Fifth label  = \", y_train[4])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First label  =  5\n",
            "Second Label =  0\n",
            "Third label  =  4\n",
            "Forth label  =  1\n",
            "Fifth label  =  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er43rsG4B07p",
        "colab_type": "text"
      },
      "source": [
        "### Data pre processing\n",
        "#### Flattern and Normalise the raw data\n",
        "#### Make sure the dimension best suit our convention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoojpm7B4fHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc69c9e2-babd-45a5-fbae-4f7ac3027ecc"
      },
      "source": [
        "m_train = x_train.shape[0] # total number of data in training set\n",
        "m_test = x_test.shape[0] # total number of data in test set\n",
        "flattern_dim = x_train.shape[1]*x_train.shape[2] # input dimension: 28 x 28 = 784\n",
        "print(m_train,m_test,flattern_dim)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 10000 784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNV78vDyB_cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape((m_train,flattern_dim)).T\n",
        "x_train = x_train / 255\n",
        "x_test = x_test.reshape((m_test,flattern_dim)).T\n",
        "x_test = x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBV59ocZ46uf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "97aab68b-8e2d-4e92-cfa1-2644a19921f0"
      },
      "source": [
        "print(x_train.shape) # Input dimension , number of data\n",
        "print(x_train.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 60000)\n",
            "(784, 60000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOa6kRWx5Tba",
        "colab_type": "text"
      },
      "source": [
        "#### Convert output lable to one_hot vector, comparable to our NN model\n",
        "#### each entry will be the probability of the corresponding class (digit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qTdd3b_5Bef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.eye(10)[y_train].T\n",
        "y_test = np.eye(10)[y_test].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7y2fms5urj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4853b704-52ea-4ad1-ef75-44f138e634c0"
      },
      "source": [
        "print(y_train.shape) # number of class, number of data\n",
        "print(y_train[:,0]) # Original: 5"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 60000)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61r_KpSWGu41",
        "colab_type": "text"
      },
      "source": [
        "## Define the network and network parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j7qLN5Nm0Jt",
        "colab_type": "text"
      },
      "source": [
        "Input feature vector : 28 x 28 = 784 dimension vector\n",
        "\n",
        "Hidden Layer = 32 neuron in hidden layer\n",
        "\n",
        ">Activation: sigmoided\n",
        "\n",
        "Output Layer = 10 neuron (class)\n",
        "\n",
        ">Activation: softmax\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsZ1b00lqTq1",
        "colab_type": "text"
      },
      "source": [
        "## Let's first define the parameters of the neuron network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmlfdolLqdR-",
        "colab_type": "text"
      },
      "source": [
        "I will first create a dictatorary to store the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7fZdiwgqOY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {}\n",
        "\n",
        "# Gaussain disctrubtion with mean 0 and variance 1\n",
        "w1 = np.random.randn(32,784)\n",
        "b1 = np.zeros((32,1))\n",
        "w2 = np.random.randn(10,32)\n",
        "b2 = np.zeros((10,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DhJqoArILZ3",
        "colab_type": "text"
      },
      "source": [
        "### Activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM9lopq3IJyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0/(1.0+np.exp(-z))\n",
        "def softmax(z):\n",
        "  return np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOZRt3p57GS-",
        "colab_type": "text"
      },
      "source": [
        "### Hyper Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajWvShh37JgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iter = 1000\n",
        "learning_rate = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65enHWeITrj",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK00YtwrGyva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "769329fc-9a0f-4900-f03d-05c7e6e5d574"
      },
      "source": [
        "for i in range(0,num_iter):\n",
        "\n",
        "  # Forward propagation\n",
        "  z1 = np.dot(w1,x_train) + b1\n",
        "  a1 = sigmoid(z1)\n",
        "  z2 = np.dot(w2,a1) + b2\n",
        "  a2 = softmax(z2)\n",
        "\n",
        "  # Loss\n",
        "  # multiclass cross entropy loss\n",
        "  loss = -1 * np.multiply(y_train, np.log(a2))\n",
        "  cost = np.sum(loss) / m_train\n",
        "\n",
        "  if(num_iter % 1 == 0):\n",
        "    print(\"iteration : {}\".format(i),end=\" \")\n",
        "    print(\"Current training loss = {}\".format(cost))\n",
        "\n",
        "  # Backward propagation\n",
        "  \n",
        "  # dSoftmax\n",
        "  dz2 = a2 - y_train\n",
        "  dw2 = (1. / m_train) * np.dot(dz2,a1.T)\n",
        "  db2 = (1. / m_train) * np.sum(dz2,axis=1,keepdims=True)\n",
        "  da1 = np.dot(w2.T,dz2)\n",
        "\n",
        "  # dSigmoid\n",
        "  dz1 = da1 * np.multiply(a1,1-a1)\n",
        "  dw1 = (1. / m_train) * np.dot(dz1,x_train.T)\n",
        "  db1 = (1. / m_train) * np.sum(dz1,axis=1,keepdims=True)\n",
        "\n",
        "  # Batch gradient descent\n",
        "  w1 = w1 - learning_rate * dw1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  w2 = w2 - learning_rate * dw2\n",
        "  b2 = b2 - learning_rate * db2"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration : 0 Current training loss = 7.573323418808963\n",
            "iteration : 1 Current training loss = 6.3173700192097275\n",
            "iteration : 2 Current training loss = 5.648786918200045\n",
            "iteration : 3 Current training loss = 5.160197488862914\n",
            "iteration : 4 Current training loss = 4.722178819516614\n",
            "iteration : 5 Current training loss = 4.339173973233745\n",
            "iteration : 6 Current training loss = 4.0318801751149325\n",
            "iteration : 7 Current training loss = 3.8046401359641795\n",
            "iteration : 8 Current training loss = 3.638835732218321\n",
            "iteration : 9 Current training loss = 3.5092281705346937\n",
            "iteration : 10 Current training loss = 3.398311954069526\n",
            "iteration : 11 Current training loss = 3.2974741629830633\n",
            "iteration : 12 Current training loss = 3.2032547415871058\n",
            "iteration : 13 Current training loss = 3.1143834432199444\n",
            "iteration : 14 Current training loss = 3.0303384319211912\n",
            "iteration : 15 Current training loss = 2.950813086090986\n",
            "iteration : 16 Current training loss = 2.8755539098055163\n",
            "iteration : 17 Current training loss = 2.804320455920787\n",
            "iteration : 18 Current training loss = 2.736880662900275\n",
            "iteration : 19 Current training loss = 2.6730139682997773\n",
            "iteration : 20 Current training loss = 2.612513434020045\n",
            "iteration : 21 Current training loss = 2.5551852450240427\n",
            "iteration : 22 Current training loss = 2.5008466613686346\n",
            "iteration : 23 Current training loss = 2.4493238728380184\n",
            "iteration : 24 Current training loss = 2.4004505737004505\n",
            "iteration : 25 Current training loss = 2.35406737873877\n",
            "iteration : 26 Current training loss = 2.310021818478642\n",
            "iteration : 27 Current training loss = 2.268168565901638\n",
            "iteration : 28 Current training loss = 2.2283696432168667\n",
            "iteration : 29 Current training loss = 2.1904945053038296\n",
            "iteration : 30 Current training loss = 2.1544199996691775\n",
            "iteration : 31 Current training loss = 2.1200302404032003\n",
            "iteration : 32 Current training loss = 2.0872164320879474\n",
            "iteration : 33 Current training loss = 2.055876665657825\n",
            "iteration : 34 Current training loss = 2.0259156947717853\n",
            "iteration : 35 Current training loss = 1.9972446928828487\n",
            "iteration : 36 Current training loss = 1.969780988606939\n",
            "iteration : 37 Current training loss = 1.943447778528136\n",
            "iteration : 38 Current training loss = 1.9181738196723408\n",
            "iteration : 39 Current training loss = 1.8938931066206413\n",
            "iteration : 40 Current training loss = 1.8705445398153806\n",
            "iteration : 41 Current training loss = 1.848071591992337\n",
            "iteration : 42 Current training loss = 1.8264219791376173\n",
            "iteration : 43 Current training loss = 1.8055473412926435\n",
            "iteration : 44 Current training loss = 1.7854029372489615\n",
            "iteration : 45 Current training loss = 1.7659473559168566\n",
            "iteration : 46 Current training loss = 1.747142246040486\n",
            "iteration : 47 Current training loss = 1.7289520650172512\n",
            "iteration : 48 Current training loss = 1.7113438468715052\n",
            "iteration : 49 Current training loss = 1.6942869889225418\n",
            "iteration : 50 Current training loss = 1.6777530563486314\n",
            "iteration : 51 Current training loss = 1.6617156036452425\n",
            "iteration : 52 Current training loss = 1.6461500118642391\n",
            "iteration : 53 Current training loss = 1.6310333404641646\n",
            "iteration : 54 Current training loss = 1.616344192573594\n",
            "iteration : 55 Current training loss = 1.602062592458342\n",
            "iteration : 56 Current training loss = 1.5881698739893841\n",
            "iteration : 57 Current training loss = 1.5746485789392086\n",
            "iteration : 58 Current training loss = 1.5614823639976505\n",
            "iteration : 59 Current training loss = 1.5486559154970405\n",
            "iteration : 60 Current training loss = 1.536154870965561\n",
            "iteration : 61 Current training loss = 1.523965746774665\n",
            "iteration : 62 Current training loss = 1.512075871294279\n",
            "iteration : 63 Current training loss = 1.5004733231016953\n",
            "iteration : 64 Current training loss = 1.4891468738941294\n",
            "iteration : 65 Current training loss = 1.4780859358254332\n",
            "iteration : 66 Current training loss = 1.467280513025714\n",
            "iteration : 67 Current training loss = 1.4567211570749954\n",
            "iteration : 68 Current training loss = 1.4463989261974932\n",
            "iteration : 69 Current training loss = 1.436305347930251\n",
            "iteration : 70 Current training loss = 1.426432385006492\n",
            "iteration : 71 Current training loss = 1.416772404185473\n",
            "iteration : 72 Current training loss = 1.4073181477609602\n",
            "iteration : 73 Current training loss = 1.398062707491289\n",
            "iteration : 74 Current training loss = 1.3889995007155262\n",
            "iteration : 75 Current training loss = 1.3801222484498954\n",
            "iteration : 76 Current training loss = 1.3714249552925424\n",
            "iteration : 77 Current training loss = 1.362901890997586\n",
            "iteration : 78 Current training loss = 1.3545475736066508\n",
            "iteration : 79 Current training loss = 1.3463567540443875\n",
            "iteration : 80 Current training loss = 1.3383244020929317\n",
            "iteration : 81 Current training loss = 1.330445693659806\n",
            "iteration : 82 Current training loss = 1.322715999247341\n",
            "iteration : 83 Current training loss = 1.315130873522561\n",
            "iteration : 84 Current training loss = 1.3076860458779498\n",
            "iteration : 85 Current training loss = 1.3003774118679459\n",
            "iteration : 86 Current training loss = 1.2932010254048614\n",
            "iteration : 87 Current training loss = 1.28615309160138\n",
            "iteration : 88 Current training loss = 1.2792299601544497\n",
            "iteration : 89 Current training loss = 1.2724281191762403\n",
            "iteration : 90 Current training loss = 1.2657441893906844\n",
            "iteration : 91 Current training loss = 1.2591749186278391\n",
            "iteration : 92 Current training loss = 1.252717176561909\n",
            "iteration : 93 Current training loss = 1.2463679496515183\n",
            "iteration : 94 Current training loss = 1.2401243362522167\n",
            "iteration : 95 Current training loss = 1.233983541881031\n",
            "iteration : 96 Current training loss = 1.2279428746209615\n",
            "iteration : 97 Current training loss = 1.2219997406598428\n",
            "iteration : 98 Current training loss = 1.2161516399628567\n",
            "iteration : 99 Current training loss = 1.2103961620816075\n",
            "iteration : 100 Current training loss = 1.2047309821049164\n",
            "iteration : 101 Current training loss = 1.1991538567577738\n",
            "iteration : 102 Current training loss = 1.1936626206551628\n",
            "iteration : 103 Current training loss = 1.1882551827170247\n",
            "iteration : 104 Current training loss = 1.182929522749498\n",
            "iteration : 105 Current training loss = 1.1776836881960517\n",
            "iteration : 106 Current training loss = 1.1725157910602442\n",
            "iteration : 107 Current training loss = 1.167424004999878\n",
            "iteration : 108 Current training loss = 1.1624065625903264\n",
            "iteration : 109 Current training loss = 1.1574617527529827\n",
            "iteration : 110 Current training loss = 1.1525879183431629\n",
            "iteration : 111 Current training loss = 1.1477834538904481\n",
            "iteration : 112 Current training loss = 1.1430468034834336\n",
            "iteration : 113 Current training loss = 1.1383764587900942\n",
            "iteration : 114 Current training loss = 1.1337709572045376\n",
            "iteration : 115 Current training loss = 1.1292288801106678\n",
            "iteration : 116 Current training loss = 1.1247488512532902\n",
            "iteration : 117 Current training loss = 1.120329535207283\n",
            "iteration : 118 Current training loss = 1.115969635935763\n",
            "iteration : 119 Current training loss = 1.1116678954285084\n",
            "iteration : 120 Current training loss = 1.107423092412322\n",
            "iteration : 121 Current training loss = 1.1032340411255412\n",
            "iteration : 122 Current training loss = 1.0990995901493819\n",
            "iteration : 123 Current training loss = 1.0950186212894268\n",
            "iteration : 124 Current training loss = 1.0909900485011053\n",
            "iteration : 125 Current training loss = 1.087012816853673\n",
            "iteration : 126 Current training loss = 1.0830859015278214\n",
            "iteration : 127 Current training loss = 1.0792083068426845\n",
            "iteration : 128 Current training loss = 1.0753790653086857\n",
            "iteration : 129 Current training loss = 1.0715972367032864\n",
            "iteration : 130 Current training loss = 1.067861907167375\n",
            "iteration : 131 Current training loss = 1.0641721883205826\n",
            "iteration : 132 Current training loss = 1.060527216394445\n",
            "iteration : 133 Current training loss = 1.0569261513827861\n",
            "iteration : 134 Current training loss = 1.0533681762091804\n",
            "iteration : 135 Current training loss = 1.04985249591171\n",
            "iteration : 136 Current training loss = 1.0463783368455244\n",
            "iteration : 137 Current training loss = 1.0429449459039146\n",
            "iteration : 138 Current training loss = 1.039551589758729\n",
            "iteration : 139 Current training loss = 1.0361975541210182\n",
            "iteration : 140 Current training loss = 1.0328821430227475\n",
            "iteration : 141 Current training loss = 1.029604678120366\n",
            "iteration : 142 Current training loss = 1.0263644980208924\n",
            "iteration : 143 Current training loss = 1.023160957631042\n",
            "iteration : 144 Current training loss = 1.0199934275297777\n",
            "iteration : 145 Current training loss = 1.0168612933645371\n",
            "iteration : 146 Current training loss = 1.0137639552712336\n",
            "iteration : 147 Current training loss = 1.0107008273180582\n",
            "iteration : 148 Current training loss = 1.0076713369729848\n",
            "iteration : 149 Current training loss = 1.004674924594834\n",
            "iteration : 150 Current training loss = 1.001711042947691\n",
            "iteration : 151 Current training loss = 0.9987791567384265\n",
            "iteration : 152 Current training loss = 0.9958787421770164\n",
            "iteration : 153 Current training loss = 0.993009286559351\n",
            "iteration : 154 Current training loss = 0.9901702878721181\n",
            "iteration : 155 Current training loss = 0.987361254419351\n",
            "iteration : 156 Current training loss = 0.9845817044701045\n",
            "iteration : 157 Current training loss = 0.9818311659267005\n",
            "iteration : 158 Current training loss = 0.97910917601286\n",
            "iteration : 159 Current training loss = 0.9764152809809785\n",
            "iteration : 160 Current training loss = 0.9737490358377106\n",
            "iteration : 161 Current training loss = 0.9711100040869478\n",
            "iteration : 162 Current training loss = 0.9684977574892096\n",
            "iteration : 163 Current training loss = 0.9659118758364122\n",
            "iteration : 164 Current training loss = 0.963351946740941\n",
            "iteration : 165 Current training loss = 0.9608175654379315\n",
            "iteration : 166 Current training loss = 0.9583083345996586\n",
            "iteration : 167 Current training loss = 0.955823864160967\n",
            "iteration : 168 Current training loss = 0.9533637711546776\n",
            "iteration : 169 Current training loss = 0.9509276795560033\n",
            "iteration : 170 Current training loss = 0.9485152201350123\n",
            "iteration : 171 Current training loss = 0.9461260303162983\n",
            "iteration : 172 Current training loss = 0.9437597540450661\n",
            "iteration : 173 Current training loss = 0.9414160416589328\n",
            "iteration : 174 Current training loss = 0.939094549764823\n",
            "iteration : 175 Current training loss = 0.936794941120415\n",
            "iteration : 176 Current training loss = 0.9345168845196724\n",
            "iteration : 177 Current training loss = 0.9322600546820525\n",
            "iteration : 178 Current training loss = 0.9300241321450617\n",
            "iteration : 179 Current training loss = 0.9278088031598501\n",
            "iteration : 180 Current training loss = 0.9256137595896158\n",
            "iteration : 181 Current training loss = 0.9234386988105956\n",
            "iteration : 182 Current training loss = 0.9212833236154622\n",
            "iteration : 183 Current training loss = 0.9191473421189656\n",
            "iteration : 184 Current training loss = 0.917030467665667\n",
            "iteration : 185 Current training loss = 0.9149324187396423\n",
            "iteration : 186 Current training loss = 0.9128529188760143\n",
            "iteration : 187 Current training loss = 0.9107916965742066\n",
            "iteration : 188 Current training loss = 0.9087484852127965\n",
            "iteration : 189 Current training loss = 0.906723022965859\n",
            "iteration : 190 Current training loss = 0.9047150527207017\n",
            "iteration : 191 Current training loss = 0.9027243219968786\n",
            "iteration : 192 Current training loss = 0.9007505828663979\n",
            "iteration : 193 Current training loss = 0.8987935918750338\n",
            "iteration : 194 Current training loss = 0.8968531099646564\n",
            "iteration : 195 Current training loss = 0.8949289023965135\n",
            "iteration : 196 Current training loss = 0.8930207386753989\n",
            "iteration : 197 Current training loss = 0.891128392474648\n",
            "iteration : 198 Current training loss = 0.8892516415619306\n",
            "iteration : 199 Current training loss = 0.8873902677257878\n",
            "iteration : 200 Current training loss = 0.8855440567029006\n",
            "iteration : 201 Current training loss = 0.8837127981060735\n",
            "iteration : 202 Current training loss = 0.8818962853529144\n",
            "iteration : 203 Current training loss = 0.880094315595226\n",
            "iteration : 204 Current training loss = 0.8783066896491014\n",
            "iteration : 205 Current training loss = 0.8765332119257442\n",
            "iteration : 206 Current training loss = 0.8747736903630265\n",
            "iteration : 207 Current training loss = 0.873027936357804\n",
            "iteration : 208 Current training loss = 0.8712957646990144\n",
            "iteration : 209 Current training loss = 0.869576993501584\n",
            "iteration : 210 Current training loss = 0.8678714441411626\n",
            "iteration : 211 Current training loss = 0.866178941189721\n",
            "iteration : 212 Current training loss = 0.8644993123520318\n",
            "iteration : 213 Current training loss = 0.8628323884030576\n",
            "iteration : 214 Current training loss = 0.8611780031262694\n",
            "iteration : 215 Current training loss = 0.8595359932529241\n",
            "iteration : 216 Current training loss = 0.8579061984023044\n",
            "iteration : 217 Current training loss = 0.8562884610229511\n",
            "iteration : 218 Current training loss = 0.8546826263348954\n",
            "iteration : 219 Current training loss = 0.8530885422729033\n",
            "iteration : 220 Current training loss = 0.8515060594307396\n",
            "iteration : 221 Current training loss = 0.8499350310064586\n",
            "iteration : 222 Current training loss = 0.8483753127487245\n",
            "iteration : 223 Current training loss = 0.8468267629041606\n",
            "iteration : 224 Current training loss = 0.8452892421657305\n",
            "iteration : 225 Current training loss = 0.8437626136221444\n",
            "iteration : 226 Current training loss = 0.8422467427082824\n",
            "iteration : 227 Current training loss = 0.8407414971566398\n",
            "iteration : 228 Current training loss = 0.8392467469497729\n",
            "iteration : 229 Current training loss = 0.837762364273741\n",
            "iteration : 230 Current training loss = 0.8362882234725407\n",
            "iteration : 231 Current training loss = 0.8348242010035083\n",
            "iteration : 232 Current training loss = 0.8333701753936866\n",
            "iteration : 233 Current training loss = 0.8319260271971463\n",
            "iteration : 234 Current training loss = 0.8304916389532359\n",
            "iteration : 235 Current training loss = 0.8290668951457588\n",
            "iteration : 236 Current training loss = 0.827651682163057\n",
            "iteration : 237 Current training loss = 0.8262458882589899\n",
            "iteration : 238 Current training loss = 0.8248494035147893\n",
            "iteration : 239 Current training loss = 0.823462119801779\n",
            "iteration : 240 Current training loss = 0.8220839307449469\n",
            "iteration : 241 Current training loss = 0.8207147316873481\n",
            "iteration : 242 Current training loss = 0.8193544196553252\n",
            "iteration : 243 Current training loss = 0.8180028933245388\n",
            "iteration : 244 Current training loss = 0.8166600529867796\n",
            "iteration : 245 Current training loss = 0.8153258005175632\n",
            "iteration : 246 Current training loss = 0.8140000393444748\n",
            "iteration : 247 Current training loss = 0.8126826744162632\n",
            "iteration : 248 Current training loss = 0.8113736121726635\n",
            "iteration : 249 Current training loss = 0.8100727605149319\n",
            "iteration : 250 Current training loss = 0.8087800287770823\n",
            "iteration : 251 Current training loss = 0.807495327697805\n",
            "iteration : 252 Current training loss = 0.8062185693930558\n",
            "iteration : 253 Current training loss = 0.804949667329299\n",
            "iteration : 254 Current training loss = 0.8036885362973917\n",
            "iteration : 255 Current training loss = 0.8024350923870928\n",
            "iteration : 256 Current training loss = 0.8011892529621816\n",
            "iteration : 257 Current training loss = 0.7999509366361729\n",
            "iteration : 258 Current training loss = 0.7987200632486143\n",
            "iteration : 259 Current training loss = 0.7974965538419492\n",
            "iteration : 260 Current training loss = 0.7962803306389327\n",
            "iteration : 261 Current training loss = 0.7950713170205841\n",
            "iteration : 262 Current training loss = 0.7938694375046668\n",
            "iteration : 263 Current training loss = 0.792674617724674\n",
            "iteration : 264 Current training loss = 0.791486784409314\n",
            "iteration : 265 Current training loss = 0.7903058653624726\n",
            "iteration : 266 Current training loss = 0.789131789443648\n",
            "iteration : 267 Current training loss = 0.7879644865488371\n",
            "iteration : 268 Current training loss = 0.7868038875918668\n",
            "iteration : 269 Current training loss = 0.785649924486147\n",
            "iteration : 270 Current training loss = 0.7845025301268448\n",
            "iteration : 271 Current training loss = 0.7833616383734554\n",
            "iteration : 272 Current training loss = 0.7822271840327666\n",
            "iteration : 273 Current training loss = 0.7810991028421971\n",
            "iteration : 274 Current training loss = 0.7799773314534998\n",
            "iteration : 275 Current training loss = 0.77886180741682\n",
            "iteration : 276 Current training loss = 0.7777524691650912\n",
            "iteration : 277 Current training loss = 0.7766492559987654\n",
            "iteration : 278 Current training loss = 0.7755521080708548\n",
            "iteration : 279 Current training loss = 0.7744609663722896\n",
            "iteration : 280 Current training loss = 0.7733757727175656\n",
            "iteration : 281 Current training loss = 0.7722964697306847\n",
            "iteration : 282 Current training loss = 0.7712230008313674\n",
            "iteration : 283 Current training loss = 0.7701553102215377\n",
            "iteration : 284 Current training loss = 0.7690933428720613\n",
            "iteration : 285 Current training loss = 0.7680370445097398\n",
            "iteration : 286 Current training loss = 0.7669863616045391\n",
            "iteration : 287 Current training loss = 0.765941241357057\n",
            "iteration : 288 Current training loss = 0.7649016316862102\n",
            "iteration : 289 Current training loss = 0.7638674812171453\n",
            "iteration : 290 Current training loss = 0.7628387392693562\n",
            "iteration : 291 Current training loss = 0.7618153558450084\n",
            "iteration : 292 Current training loss = 0.7607972816174599\n",
            "iteration : 293 Current training loss = 0.7597844679199761\n",
            "iteration : 294 Current training loss = 0.7587768667346297\n",
            "iteration : 295 Current training loss = 0.7577744306813852\n",
            "iteration : 296 Current training loss = 0.7567771130073576\n",
            "iteration : 297 Current training loss = 0.7557848675762486\n",
            "iteration : 298 Current training loss = 0.7547976488579454\n",
            "iteration : 299 Current training loss = 0.7538154119182898\n",
            "iteration : 300 Current training loss = 0.7528381124090103\n",
            "iteration : 301 Current training loss = 0.7518657065578079\n",
            "iteration : 302 Current training loss = 0.7508981511586056\n",
            "iteration : 303 Current training loss = 0.7499354035619465\n",
            "iteration : 304 Current training loss = 0.748977421665548\n",
            "iteration : 305 Current training loss = 0.7480241639050053\n",
            "iteration : 306 Current training loss = 0.7470755892446436\n",
            "iteration : 307 Current training loss = 0.7461316571685196\n",
            "iteration : 308 Current training loss = 0.7451923276715675\n",
            "iteration : 309 Current training loss = 0.744257561250895\n",
            "iteration : 310 Current training loss = 0.7433273188972181\n",
            "iteration : 311 Current training loss = 0.7424015620864484\n",
            "iteration : 312 Current training loss = 0.7414802527714162\n",
            "iteration : 313 Current training loss = 0.7405633533737437\n",
            "iteration : 314 Current training loss = 0.7396508267758592\n",
            "iteration : 315 Current training loss = 0.7387426363131532\n",
            "iteration : 316 Current training loss = 0.7378387457662786\n",
            "iteration : 317 Current training loss = 0.7369391193535945\n",
            "iteration : 318 Current training loss = 0.7360437217237473\n",
            "iteration : 319 Current training loss = 0.7351525179484043\n",
            "iteration : 320 Current training loss = 0.7342654735151124\n",
            "iteration : 321 Current training loss = 0.7333825543203161\n",
            "iteration : 322 Current training loss = 0.7325037266625027\n",
            "iteration : 323 Current training loss = 0.731628957235492\n",
            "iteration : 324 Current training loss = 0.7307582131218674\n",
            "iteration : 325 Current training loss = 0.7298914617865423\n",
            "iteration : 326 Current training loss = 0.7290286710704633\n",
            "iteration : 327 Current training loss = 0.7281698091844536\n",
            "iteration : 328 Current training loss = 0.7273148447031876\n",
            "iteration : 329 Current training loss = 0.7264637465593006\n",
            "iteration : 330 Current training loss = 0.7256164840376288\n",
            "iteration : 331 Current training loss = 0.724773026769584\n",
            "iteration : 332 Current training loss = 0.7239333447276527\n",
            "iteration : 333 Current training loss = 0.7230974082200212\n",
            "iteration : 334 Current training loss = 0.7222651878853303\n",
            "iteration : 335 Current training loss = 0.721436654687547\n",
            "iteration : 336 Current training loss = 0.7206117799109591\n",
            "iteration : 337 Current training loss = 0.7197905351552865\n",
            "iteration : 338 Current training loss = 0.7189728923309079\n",
            "iteration : 339 Current training loss = 0.7181588236541979\n",
            "iteration : 340 Current training loss = 0.7173483016429785\n",
            "iteration : 341 Current training loss = 0.7165412991120732\n",
            "iteration : 342 Current training loss = 0.7157377891689666\n",
            "iteration : 343 Current training loss = 0.7149377452095673\n",
            "iteration : 344 Current training loss = 0.7141411409140658\n",
            "iteration : 345 Current training loss = 0.7133479502428924\n",
            "iteration : 346 Current training loss = 0.7125581474327636\n",
            "iteration : 347 Current training loss = 0.7117717069928227\n",
            "iteration : 348 Current training loss = 0.7109886037008613\n",
            "iteration : 349 Current training loss = 0.710208812599634\n",
            "iteration : 350 Current training loss = 0.7094323089932453\n",
            "iteration : 351 Current training loss = 0.7086590684436227\n",
            "iteration : 352 Current training loss = 0.7078890667670582\n",
            "iteration : 353 Current training loss = 0.7071222800308313\n",
            "iteration : 354 Current training loss = 0.7063586845498966\n",
            "iteration : 355 Current training loss = 0.7055982568836406\n",
            "iteration : 356 Current training loss = 0.7048409738327082\n",
            "iteration : 357 Current training loss = 0.7040868124358862\n",
            "iteration : 358 Current training loss = 0.7033357499670541\n",
            "iteration : 359 Current training loss = 0.702587763932188\n",
            "iteration : 360 Current training loss = 0.7018428320664274\n",
            "iteration : 361 Current training loss = 0.7011009323311888\n",
            "iteration : 362 Current training loss = 0.7003620429113416\n",
            "iteration : 363 Current training loss = 0.6996261422124271\n",
            "iteration : 364 Current training loss = 0.6988932088579278\n",
            "iteration : 365 Current training loss = 0.6981632216865898\n",
            "iteration : 366 Current training loss = 0.6974361597497833\n",
            "iteration : 367 Current training loss = 0.69671200230891\n",
            "iteration : 368 Current training loss = 0.6959907288328578\n",
            "iteration : 369 Current training loss = 0.6952723189954898\n",
            "iteration : 370 Current training loss = 0.6945567526731777\n",
            "iteration : 371 Current training loss = 0.6938440099423722\n",
            "iteration : 372 Current training loss = 0.693134071077212\n",
            "iteration : 373 Current training loss = 0.6924269165471697\n",
            "iteration : 374 Current training loss = 0.6917225270147304\n",
            "iteration : 375 Current training loss = 0.6910208833331076\n",
            "iteration : 376 Current training loss = 0.6903219665439921\n",
            "iteration : 377 Current training loss = 0.6896257578753319\n",
            "iteration : 378 Current training loss = 0.6889322387391448\n",
            "iteration : 379 Current training loss = 0.6882413907293626\n",
            "iteration : 380 Current training loss = 0.6875531956197023\n",
            "iteration : 381 Current training loss = 0.6868676353615724\n",
            "iteration : 382 Current training loss = 0.6861846920820012\n",
            "iteration : 383 Current training loss = 0.6855043480815959\n",
            "iteration : 384 Current training loss = 0.6848265858325319\n",
            "iteration : 385 Current training loss = 0.6841513879765637\n",
            "iteration : 386 Current training loss = 0.6834787373230637\n",
            "iteration : 387 Current training loss = 0.6828086168470888\n",
            "iteration : 388 Current training loss = 0.6821410096874693\n",
            "iteration : 389 Current training loss = 0.6814758991449208\n",
            "iteration : 390 Current training loss = 0.6808132686801852\n",
            "iteration : 391 Current training loss = 0.6801531019121911\n",
            "iteration : 392 Current training loss = 0.6794953826162357\n",
            "iteration : 393 Current training loss = 0.6788400947221942\n",
            "iteration : 394 Current training loss = 0.6781872223127472\n",
            "iteration : 395 Current training loss = 0.6775367496216308\n",
            "iteration : 396 Current training loss = 0.6768886610319084\n",
            "iteration : 397 Current training loss = 0.6762429410742614\n",
            "iteration : 398 Current training loss = 0.6755995744253043\n",
            "iteration : 399 Current training loss = 0.6749585459059149\n",
            "iteration : 400 Current training loss = 0.6743198404795859\n",
            "iteration : 401 Current training loss = 0.6736834432507987\n",
            "iteration : 402 Current training loss = 0.6730493394634113\n",
            "iteration : 403 Current training loss = 0.6724175144990694\n",
            "iteration : 404 Current training loss = 0.6717879538756313\n",
            "iteration : 405 Current training loss = 0.671160643245614\n",
            "iteration : 406 Current training loss = 0.6705355683946568\n",
            "iteration : 407 Current training loss = 0.6699127152400013\n",
            "iteration : 408 Current training loss = 0.6692920698289876\n",
            "iteration : 409 Current training loss = 0.6686736183375716\n",
            "iteration : 410 Current training loss = 0.6680573470688522\n",
            "iteration : 411 Current training loss = 0.6674432424516222\n",
            "iteration : 412 Current training loss = 0.6668312910389317\n",
            "iteration : 413 Current training loss = 0.6662214795066652\n",
            "iteration : 414 Current training loss = 0.6656137946521401\n",
            "iteration : 415 Current training loss = 0.6650082233927171\n",
            "iteration : 416 Current training loss = 0.6644047527644259\n",
            "iteration : 417 Current training loss = 0.6638033699206085\n",
            "iteration : 418 Current training loss = 0.6632040621305757\n",
            "iteration : 419 Current training loss = 0.6626068167782776\n",
            "iteration : 420 Current training loss = 0.6620116213609935\n",
            "iteration : 421 Current training loss = 0.6614184634880308\n",
            "iteration : 422 Current training loss = 0.6608273308794419\n",
            "iteration : 423 Current training loss = 0.6602382113647534\n",
            "iteration : 424 Current training loss = 0.6596510928817129\n",
            "iteration : 425 Current training loss = 0.6590659634750454\n",
            "iteration : 426 Current training loss = 0.6584828112952282\n",
            "iteration : 427 Current training loss = 0.6579016245972761\n",
            "iteration : 428 Current training loss = 0.6573223917395415\n",
            "iteration : 429 Current training loss = 0.6567451011825315\n",
            "iteration : 430 Current training loss = 0.6561697414877323\n",
            "iteration : 431 Current training loss = 0.655596301316453\n",
            "iteration : 432 Current training loss = 0.6550247694286794\n",
            "iteration : 433 Current training loss = 0.6544551346819432\n",
            "iteration : 434 Current training loss = 0.653887386030201\n",
            "iteration : 435 Current training loss = 0.6533215125227329\n",
            "iteration : 436 Current training loss = 0.6527575033030465\n",
            "iteration : 437 Current training loss = 0.6521953476077991\n",
            "iteration : 438 Current training loss = 0.6516350347657321\n",
            "iteration : 439 Current training loss = 0.6510765541966163\n",
            "iteration : 440 Current training loss = 0.6505198954102102\n",
            "iteration : 441 Current training loss = 0.6499650480052344\n",
            "iteration : 442 Current training loss = 0.6494120016683524\n",
            "iteration : 443 Current training loss = 0.6488607461731701\n",
            "iteration : 444 Current training loss = 0.6483112713792444\n",
            "iteration : 445 Current training loss = 0.6477635672311023\n",
            "iteration : 446 Current training loss = 0.6472176237572776\n",
            "iteration : 447 Current training loss = 0.646673431069354\n",
            "iteration : 448 Current training loss = 0.6461309793610231\n",
            "iteration : 449 Current training loss = 0.6455902589071549\n",
            "iteration : 450 Current training loss = 0.6450512600628776\n",
            "iteration : 451 Current training loss = 0.6445139732626706\n",
            "iteration : 452 Current training loss = 0.6439783890194691\n",
            "iteration : 453 Current training loss = 0.6434444979237797\n",
            "iteration : 454 Current training loss = 0.6429122906428081\n",
            "iteration : 455 Current training loss = 0.6423817579195956\n",
            "iteration : 456 Current training loss = 0.6418528905721711\n",
            "iteration : 457 Current training loss = 0.6413256794927095\n",
            "iteration : 458 Current training loss = 0.6408001156467045\n",
            "iteration : 459 Current training loss = 0.6402761900721495\n",
            "iteration : 460 Current training loss = 0.6397538938787316\n",
            "iteration : 461 Current training loss = 0.639233218247034\n",
            "iteration : 462 Current training loss = 0.6387141544277508\n",
            "iteration : 463 Current training loss = 0.638196693740911\n",
            "iteration : 464 Current training loss = 0.6376808275751117\n",
            "iteration : 465 Current training loss = 0.6371665473867649\n",
            "iteration : 466 Current training loss = 0.6366538446993503\n",
            "iteration : 467 Current training loss = 0.6361427111026801\n",
            "iteration : 468 Current training loss = 0.6356331382521736\n",
            "iteration : 469 Current training loss = 0.6351251178681412\n",
            "iteration : 470 Current training loss = 0.6346186417350766\n",
            "iteration : 471 Current training loss = 0.6341137017009612\n",
            "iteration : 472 Current training loss = 0.6336102896765748\n",
            "iteration : 473 Current training loss = 0.633108397634818\n",
            "iteration : 474 Current training loss = 0.6326080176100419\n",
            "iteration : 475 Current training loss = 0.6321091416973895\n",
            "iteration : 476 Current training loss = 0.6316117620521386\n",
            "iteration : 477 Current training loss = 0.6311158708890655\n",
            "iteration : 478 Current training loss = 0.6306214604818063\n",
            "iteration : 479 Current training loss = 0.6301285231622319\n",
            "iteration : 480 Current training loss = 0.6296370513198308\n",
            "iteration : 481 Current training loss = 0.6291470374011\n",
            "iteration : 482 Current training loss = 0.6286584739089442\n",
            "iteration : 483 Current training loss = 0.6281713534020809\n",
            "iteration : 484 Current training loss = 0.6276856684944592\n",
            "iteration : 485 Current training loss = 0.6272014118546795\n",
            "iteration : 486 Current training loss = 0.6267185762054234\n",
            "iteration : 487 Current training loss = 0.6262371543228958\n",
            "iteration : 488 Current training loss = 0.6257571390362682\n",
            "iteration : 489 Current training loss = 0.6252785232271312\n",
            "iteration : 490 Current training loss = 0.6248012998289569\n",
            "iteration : 491 Current training loss = 0.624325461826565\n",
            "iteration : 492 Current training loss = 0.6238510022556003\n",
            "iteration : 493 Current training loss = 0.6233779142020104\n",
            "iteration : 494 Current training loss = 0.6229061908015383\n",
            "iteration : 495 Current training loss = 0.622435825239217\n",
            "iteration : 496 Current training loss = 0.6219668107488702\n",
            "iteration : 497 Current training loss = 0.6214991406126232\n",
            "iteration : 498 Current training loss = 0.6210328081604183\n",
            "iteration : 499 Current training loss = 0.6205678067695353\n",
            "iteration : 500 Current training loss = 0.6201041298641203\n",
            "iteration : 501 Current training loss = 0.6196417709147216\n",
            "iteration : 502 Current training loss = 0.6191807234378286\n",
            "iteration : 503 Current training loss = 0.6187209809954182\n",
            "iteration : 504 Current training loss = 0.6182625371945093\n",
            "iteration : 505 Current training loss = 0.6178053856867197\n",
            "iteration : 506 Current training loss = 0.6173495201678316\n",
            "iteration : 507 Current training loss = 0.6168949343773603\n",
            "iteration : 508 Current training loss = 0.6164416220981311\n",
            "iteration : 509 Current training loss = 0.6159895771558596\n",
            "iteration : 510 Current training loss = 0.6155387934187385\n",
            "iteration : 511 Current training loss = 0.6150892647970293\n",
            "iteration : 512 Current training loss = 0.6146409852426623\n",
            "iteration : 513 Current training loss = 0.614193948748834\n",
            "iteration : 514 Current training loss = 0.6137481493496193\n",
            "iteration : 515 Current training loss = 0.6133035811195828\n",
            "iteration : 516 Current training loss = 0.6128602381733955\n",
            "iteration : 517 Current training loss = 0.612418114665459\n",
            "iteration : 518 Current training loss = 0.6119772047895324\n",
            "iteration : 519 Current training loss = 0.6115375027783643\n",
            "iteration : 520 Current training loss = 0.6110990029033311\n",
            "iteration : 521 Current training loss = 0.6106616994740774\n",
            "iteration : 522 Current training loss = 0.6102255868381633\n",
            "iteration : 523 Current training loss = 0.6097906593807142\n",
            "iteration : 524 Current training loss = 0.609356911524078\n",
            "iteration : 525 Current training loss = 0.6089243377274822\n",
            "iteration : 526 Current training loss = 0.6084929324867004\n",
            "iteration : 527 Current training loss = 0.6080626903337207\n",
            "iteration : 528 Current training loss = 0.6076336058364159\n",
            "iteration : 529 Current training loss = 0.607205673598224\n",
            "iteration : 530 Current training loss = 0.6067788882578248\n",
            "iteration : 531 Current training loss = 0.6063532444888301\n",
            "iteration : 532 Current training loss = 0.6059287369994688\n",
            "iteration : 533 Current training loss = 0.6055053605322813\n",
            "iteration : 534 Current training loss = 0.6050831098638179\n",
            "iteration : 535 Current training loss = 0.6046619798043367\n",
            "iteration : 536 Current training loss = 0.6042419651975114\n",
            "iteration : 537 Current training loss = 0.6038230609201382\n",
            "iteration : 538 Current training loss = 0.6034052618818487\n",
            "iteration : 539 Current training loss = 0.6029885630248252\n",
            "iteration : 540 Current training loss = 0.6025729593235218\n",
            "iteration : 541 Current training loss = 0.6021584457843847\n",
            "iteration : 542 Current training loss = 0.6017450174455832\n",
            "iteration : 543 Current training loss = 0.6013326693767368\n",
            "iteration : 544 Current training loss = 0.6009213966786483\n",
            "iteration : 545 Current training loss = 0.6005111944830451\n",
            "iteration : 546 Current training loss = 0.6001020579523162\n",
            "iteration : 547 Current training loss = 0.5996939822792564\n",
            "iteration : 548 Current training loss = 0.5992869626868156\n",
            "iteration : 549 Current training loss = 0.5988809944278473\n",
            "iteration : 550 Current training loss = 0.598476072784862\n",
            "iteration : 551 Current training loss = 0.5980721930697868\n",
            "iteration : 552 Current training loss = 0.5976693506237228\n",
            "iteration : 553 Current training loss = 0.5972675408167085\n",
            "iteration : 554 Current training loss = 0.5968667590474868\n",
            "iteration : 555 Current training loss = 0.5964670007432733\n",
            "iteration : 556 Current training loss = 0.5960682613595288\n",
            "iteration : 557 Current training loss = 0.5956705363797345\n",
            "iteration : 558 Current training loss = 0.5952738213151694\n",
            "iteration : 559 Current training loss = 0.5948781117046896\n",
            "iteration : 560 Current training loss = 0.5944834031145162\n",
            "iteration : 561 Current training loss = 0.5940896911380162\n",
            "iteration : 562 Current training loss = 0.5936969713954946\n",
            "iteration : 563 Current training loss = 0.5933052395339846\n",
            "iteration : 564 Current training loss = 0.5929144912270432\n",
            "iteration : 565 Current training loss = 0.5925247221745458\n",
            "iteration : 566 Current training loss = 0.5921359281024884\n",
            "iteration : 567 Current training loss = 0.5917481047627862\n",
            "iteration : 568 Current training loss = 0.5913612479330799\n",
            "iteration : 569 Current training loss = 0.590975353416542\n",
            "iteration : 570 Current training loss = 0.5905904170416842\n",
            "iteration : 571 Current training loss = 0.5902064346621723\n",
            "iteration : 572 Current training loss = 0.5898234021566348\n",
            "iteration : 573 Current training loss = 0.5894413154284831\n",
            "iteration : 574 Current training loss = 0.5890601704057259\n",
            "iteration : 575 Current training loss = 0.5886799630407932\n",
            "iteration : 576 Current training loss = 0.5883006893103537\n",
            "iteration : 577 Current training loss = 0.587922345215143\n",
            "iteration : 578 Current training loss = 0.5875449267797882\n",
            "iteration : 579 Current training loss = 0.5871684300526361\n",
            "iteration : 580 Current training loss = 0.5867928511055829\n",
            "iteration : 581 Current training loss = 0.5864181860339076\n",
            "iteration : 582 Current training loss = 0.5860444309561045\n",
            "iteration : 583 Current training loss = 0.5856715820137189\n",
            "iteration : 584 Current training loss = 0.5852996353711852\n",
            "iteration : 585 Current training loss = 0.5849285872156654\n",
            "iteration : 586 Current training loss = 0.5845584337568902\n",
            "iteration : 587 Current training loss = 0.5841891712270008\n",
            "iteration : 588 Current training loss = 0.583820795880393\n",
            "iteration : 589 Current training loss = 0.5834533039935638\n",
            "iteration : 590 Current training loss = 0.5830866918649567\n",
            "iteration : 591 Current training loss = 0.5827209558148113\n",
            "iteration : 592 Current training loss = 0.582356092185012\n",
            "iteration : 593 Current training loss = 0.5819920973389416\n",
            "iteration : 594 Current training loss = 0.581628967661332\n",
            "iteration : 595 Current training loss = 0.5812666995581177\n",
            "iteration : 596 Current training loss = 0.5809052894562934\n",
            "iteration : 597 Current training loss = 0.5805447338037679\n",
            "iteration : 598 Current training loss = 0.5801850290692246\n",
            "iteration : 599 Current training loss = 0.5798261717419771\n",
            "iteration : 600 Current training loss = 0.579468158331832\n",
            "iteration : 601 Current training loss = 0.5791109853689492\n",
            "iteration : 602 Current training loss = 0.5787546494037032\n",
            "iteration : 603 Current training loss = 0.5783991470065478\n",
            "iteration : 604 Current training loss = 0.5780444747678795\n",
            "iteration : 605 Current training loss = 0.5776906292979033\n",
            "iteration : 606 Current training loss = 0.5773376072264986\n",
            "iteration : 607 Current training loss = 0.5769854052030864\n",
            "iteration : 608 Current training loss = 0.5766340198964982\n",
            "iteration : 609 Current training loss = 0.5762834479948437\n",
            "iteration : 610 Current training loss = 0.5759336862053822\n",
            "iteration : 611 Current training loss = 0.575584731254392\n",
            "iteration : 612 Current training loss = 0.5752365798870425\n",
            "iteration : 613 Current training loss = 0.5748892288672663\n",
            "iteration : 614 Current training loss = 0.5745426749776338\n",
            "iteration : 615 Current training loss = 0.5741969150192233\n",
            "iteration : 616 Current training loss = 0.5738519458114999\n",
            "iteration : 617 Current training loss = 0.5735077641921885\n",
            "iteration : 618 Current training loss = 0.5731643670171498\n",
            "iteration : 619 Current training loss = 0.5728217511602576\n",
            "iteration : 620 Current training loss = 0.5724799135132763\n",
            "iteration : 621 Current training loss = 0.5721388509857375\n",
            "iteration : 622 Current training loss = 0.5717985605048208\n",
            "iteration : 623 Current training loss = 0.5714590390152307\n",
            "iteration : 624 Current training loss = 0.5711202834790788\n",
            "iteration : 625 Current training loss = 0.5707822908757622\n",
            "iteration : 626 Current training loss = 0.5704450582018457\n",
            "iteration : 627 Current training loss = 0.5701085824709439\n",
            "iteration : 628 Current training loss = 0.5697728607136013\n",
            "iteration : 629 Current training loss = 0.5694378899771775\n",
            "iteration : 630 Current training loss = 0.5691036673257291\n",
            "iteration : 631 Current training loss = 0.5687701898398944\n",
            "iteration : 632 Current training loss = 0.5684374546167759\n",
            "iteration : 633 Current training loss = 0.5681054587698288\n",
            "iteration : 634 Current training loss = 0.5677741994287429\n",
            "iteration : 635 Current training loss = 0.5674436737393298\n",
            "iteration : 636 Current training loss = 0.5671138788634116\n",
            "iteration : 637 Current training loss = 0.566784811978704\n",
            "iteration : 638 Current training loss = 0.5664564702787069\n",
            "iteration : 639 Current training loss = 0.5661288509725922\n",
            "iteration : 640 Current training loss = 0.5658019512850906\n",
            "iteration : 641 Current training loss = 0.5654757684563825\n",
            "iteration : 642 Current training loss = 0.5651502997419874\n",
            "iteration : 643 Current training loss = 0.5648255424126531\n",
            "iteration : 644 Current training loss = 0.564501493754248\n",
            "iteration : 645 Current training loss = 0.5641781510676507\n",
            "iteration : 646 Current training loss = 0.563855511668643\n",
            "iteration : 647 Current training loss = 0.5635335728878018\n",
            "iteration : 648 Current training loss = 0.5632123320703919\n",
            "iteration : 649 Current training loss = 0.5628917865762585\n",
            "iteration : 650 Current training loss = 0.5625719337797234\n",
            "iteration : 651 Current training loss = 0.5622527710694765\n",
            "iteration : 652 Current training loss = 0.5619342958484742\n",
            "iteration : 653 Current training loss = 0.5616165055338314\n",
            "iteration : 654 Current training loss = 0.5612993975567201\n",
            "iteration : 655 Current training loss = 0.5609829693622653\n",
            "iteration : 656 Current training loss = 0.5606672184094427\n",
            "iteration : 657 Current training loss = 0.5603521421709754\n",
            "iteration : 658 Current training loss = 0.5600377381332332\n",
            "iteration : 659 Current training loss = 0.5597240037961326\n",
            "iteration : 660 Current training loss = 0.5594109366730342\n",
            "iteration : 661 Current training loss = 0.5590985342906446\n",
            "iteration : 662 Current training loss = 0.5587867941889165\n",
            "iteration : 663 Current training loss = 0.5584757139209507\n",
            "iteration : 664 Current training loss = 0.5581652910528977\n",
            "iteration : 665 Current training loss = 0.5578555231638602\n",
            "iteration : 666 Current training loss = 0.5575464078457972\n",
            "iteration : 667 Current training loss = 0.5572379427034279\n",
            "iteration : 668 Current training loss = 0.5569301253541352\n",
            "iteration : 669 Current training loss = 0.556622953427872\n",
            "iteration : 670 Current training loss = 0.5563164245670683\n",
            "iteration : 671 Current training loss = 0.5560105364265343\n",
            "iteration : 672 Current training loss = 0.5557052866733715\n",
            "iteration : 673 Current training loss = 0.5554006729868787\n",
            "iteration : 674 Current training loss = 0.5550966930584608\n",
            "iteration : 675 Current training loss = 0.554793344591539\n",
            "iteration : 676 Current training loss = 0.5544906253014595\n",
            "iteration : 677 Current training loss = 0.5541885329154055\n",
            "iteration : 678 Current training loss = 0.5538870651723083\n",
            "iteration : 679 Current training loss = 0.5535862198227598\n",
            "iteration : 680 Current training loss = 0.5532859946289247\n",
            "iteration : 681 Current training loss = 0.5529863873644542\n",
            "iteration : 682 Current training loss = 0.5526873958144015\n",
            "iteration : 683 Current training loss = 0.5523890177751359\n",
            "iteration : 684 Current training loss = 0.5520912510542586\n",
            "iteration : 685 Current training loss = 0.5517940934705196\n",
            "iteration : 686 Current training loss = 0.551497542853736\n",
            "iteration : 687 Current training loss = 0.5512015970447082\n",
            "iteration : 688 Current training loss = 0.5509062538951404\n",
            "iteration : 689 Current training loss = 0.550611511267559\n",
            "iteration : 690 Current training loss = 0.5503173670352345\n",
            "iteration : 691 Current training loss = 0.5500238190821006\n",
            "iteration : 692 Current training loss = 0.5497308653026779\n",
            "iteration : 693 Current training loss = 0.5494385036019961\n",
            "iteration : 694 Current training loss = 0.549146731895517\n",
            "iteration : 695 Current training loss = 0.5488555481090583\n",
            "iteration : 696 Current training loss = 0.5485649501787209\n",
            "iteration : 697 Current training loss = 0.5482749360508115\n",
            "iteration : 698 Current training loss = 0.5479855036817719\n",
            "iteration : 699 Current training loss = 0.5476966510381035\n",
            "iteration : 700 Current training loss = 0.5474083760962988\n",
            "iteration : 701 Current training loss = 0.5471206768427671\n",
            "iteration : 702 Current training loss = 0.5468335512737662\n",
            "iteration : 703 Current training loss = 0.5465469973953316\n",
            "iteration : 704 Current training loss = 0.5462610132232081\n",
            "iteration : 705 Current training loss = 0.5459755967827817\n",
            "iteration : 706 Current training loss = 0.5456907461090126\n",
            "iteration : 707 Current training loss = 0.545406459246368\n",
            "iteration : 708 Current training loss = 0.5451227342487569\n",
            "iteration : 709 Current training loss = 0.5448395691794644\n",
            "iteration : 710 Current training loss = 0.5445569621110883\n",
            "iteration : 711 Current training loss = 0.5442749111254749\n",
            "iteration : 712 Current training loss = 0.5439934143136562\n",
            "iteration : 713 Current training loss = 0.5437124697757887\n",
            "iteration : 714 Current training loss = 0.5434320756210902\n",
            "iteration : 715 Current training loss = 0.543152229967782\n",
            "iteration : 716 Current training loss = 0.5428729309430268\n",
            "iteration : 717 Current training loss = 0.5425941766828698\n",
            "iteration : 718 Current training loss = 0.5423159653321813\n",
            "iteration : 719 Current training loss = 0.5420382950445988\n",
            "iteration : 720 Current training loss = 0.5417611639824692\n",
            "iteration : 721 Current training loss = 0.5414845703167934\n",
            "iteration : 722 Current training loss = 0.5412085122271697\n",
            "iteration : 723 Current training loss = 0.5409329879017403\n",
            "iteration : 724 Current training loss = 0.5406579955371358\n",
            "iteration : 725 Current training loss = 0.5403835333384222\n",
            "iteration : 726 Current training loss = 0.540109599519048\n",
            "iteration : 727 Current training loss = 0.5398361923007924\n",
            "iteration : 728 Current training loss = 0.5395633099137127\n",
            "iteration : 729 Current training loss = 0.5392909505960934\n",
            "iteration : 730 Current training loss = 0.5390191125943982\n",
            "iteration : 731 Current training loss = 0.5387477941632173\n",
            "iteration : 732 Current training loss = 0.5384769935652195\n",
            "iteration : 733 Current training loss = 0.5382067090711049\n",
            "iteration : 734 Current training loss = 0.5379369389595556\n",
            "iteration : 735 Current training loss = 0.5376676815171891\n",
            "iteration : 736 Current training loss = 0.5373989350385113\n",
            "iteration : 737 Current training loss = 0.5371306978258713\n",
            "iteration : 738 Current training loss = 0.5368629681894151\n",
            "iteration : 739 Current training loss = 0.5365957444470408\n",
            "iteration : 740 Current training loss = 0.5363290249243549\n",
            "iteration : 741 Current training loss = 0.5360628079546268\n",
            "iteration : 742 Current training loss = 0.5357970918787482\n",
            "iteration : 743 Current training loss = 0.5355318750451881\n",
            "iteration : 744 Current training loss = 0.5352671558099509\n",
            "iteration : 745 Current training loss = 0.5350029325365354\n",
            "iteration : 746 Current training loss = 0.5347392035958932\n",
            "iteration : 747 Current training loss = 0.5344759673663879\n",
            "iteration : 748 Current training loss = 0.5342132222337549\n",
            "iteration : 749 Current training loss = 0.533950966591061\n",
            "iteration : 750 Current training loss = 0.5336891988386667\n",
            "iteration : 751 Current training loss = 0.5334279173841849\n",
            "iteration : 752 Current training loss = 0.5331671206424445\n",
            "iteration : 753 Current training loss = 0.5329068070354509\n",
            "iteration : 754 Current training loss = 0.5326469749923504\n",
            "iteration : 755 Current training loss = 0.5323876229493897\n",
            "iteration : 756 Current training loss = 0.5321287493498823\n",
            "iteration : 757 Current training loss = 0.5318703526441704\n",
            "iteration : 758 Current training loss = 0.5316124312895896\n",
            "iteration : 759 Current training loss = 0.5313549837504322\n",
            "iteration : 760 Current training loss = 0.5310980084979129\n",
            "iteration : 761 Current training loss = 0.5308415040101335\n",
            "iteration : 762 Current training loss = 0.5305854687720484\n",
            "iteration : 763 Current training loss = 0.5303299012754298\n",
            "iteration : 764 Current training loss = 0.5300748000188342\n",
            "iteration : 765 Current training loss = 0.5298201635075688\n",
            "iteration : 766 Current training loss = 0.5295659902536577\n",
            "iteration : 767 Current training loss = 0.5293122787758086\n",
            "iteration : 768 Current training loss = 0.5290590275993821\n",
            "iteration : 769 Current training loss = 0.5288062352563558\n",
            "iteration : 770 Current training loss = 0.5285539002852949\n",
            "iteration : 771 Current training loss = 0.5283020212313185\n",
            "iteration : 772 Current training loss = 0.5280505966460689\n",
            "iteration : 773 Current training loss = 0.5277996250876797\n",
            "iteration : 774 Current training loss = 0.5275491051207439\n",
            "iteration : 775 Current training loss = 0.5272990353162844\n",
            "iteration : 776 Current training loss = 0.5270494142517212\n",
            "iteration : 777 Current training loss = 0.5268002405108422\n",
            "iteration : 778 Current training loss = 0.526551512683772\n",
            "iteration : 779 Current training loss = 0.5263032293669425\n",
            "iteration : 780 Current training loss = 0.5260553891630609\n",
            "iteration : 781 Current training loss = 0.5258079906810826\n",
            "iteration : 782 Current training loss = 0.5255610325361793\n",
            "iteration : 783 Current training loss = 0.525314513349709\n",
            "iteration : 784 Current training loss = 0.5250684317491886\n",
            "iteration : 785 Current training loss = 0.5248227863682633\n",
            "iteration : 786 Current training loss = 0.5245775758466774\n",
            "iteration : 787 Current training loss = 0.5243327988302453\n",
            "iteration : 788 Current training loss = 0.5240884539708226\n",
            "iteration : 789 Current training loss = 0.5238445399262776\n",
            "iteration : 790 Current training loss = 0.5236010553604613\n",
            "iteration : 791 Current training loss = 0.5233579989431812\n",
            "iteration : 792 Current training loss = 0.5231153693501697\n",
            "iteration : 793 Current training loss = 0.5228731652630576\n",
            "iteration : 794 Current training loss = 0.5226313853693453\n",
            "iteration : 795 Current training loss = 0.5223900283623742\n",
            "iteration : 796 Current training loss = 0.5221490929412989\n",
            "iteration : 797 Current training loss = 0.5219085778110581\n",
            "iteration : 798 Current training loss = 0.5216684816823478\n",
            "iteration : 799 Current training loss = 0.5214288032715914\n",
            "iteration : 800 Current training loss = 0.521189541300913\n",
            "iteration : 801 Current training loss = 0.5209506944981097\n",
            "iteration : 802 Current training loss = 0.5207122615966215\n",
            "iteration : 803 Current training loss = 0.5204742413355067\n",
            "iteration : 804 Current training loss = 0.52023663245941\n",
            "iteration : 805 Current training loss = 0.5199994337185381\n",
            "iteration : 806 Current training loss = 0.5197626438686296\n",
            "iteration : 807 Current training loss = 0.5195262616709279\n",
            "iteration : 808 Current training loss = 0.5192902858921539\n",
            "iteration : 809 Current training loss = 0.5190547153044764\n",
            "iteration : 810 Current training loss = 0.518819548685487\n",
            "iteration : 811 Current training loss = 0.5185847848181696\n",
            "iteration : 812 Current training loss = 0.5183504224908734\n",
            "iteration : 813 Current training loss = 0.518116460497286\n",
            "iteration : 814 Current training loss = 0.5178828976364052\n",
            "iteration : 815 Current training loss = 0.5176497327125097\n",
            "iteration : 816 Current training loss = 0.5174169645351328\n",
            "iteration : 817 Current training loss = 0.5171845919190351\n",
            "iteration : 818 Current training loss = 0.5169526136841743\n",
            "iteration : 819 Current training loss = 0.5167210286556791\n",
            "iteration : 820 Current training loss = 0.5164898356638212\n",
            "iteration : 821 Current training loss = 0.5162590335439875\n",
            "iteration : 822 Current training loss = 0.5160286211366512\n",
            "iteration : 823 Current training loss = 0.5157985972873438\n",
            "iteration : 824 Current training loss = 0.5155689608466303\n",
            "iteration : 825 Current training loss = 0.515339710670076\n",
            "iteration : 826 Current training loss = 0.5151108456182237\n",
            "iteration : 827 Current training loss = 0.5148823645565618\n",
            "iteration : 828 Current training loss = 0.5146542663554997\n",
            "iteration : 829 Current training loss = 0.5144265498903369\n",
            "iteration : 830 Current training loss = 0.5141992140412365\n",
            "iteration : 831 Current training loss = 0.513972257693198\n",
            "iteration : 832 Current training loss = 0.5137456797360274\n",
            "iteration : 833 Current training loss = 0.5135194790643102\n",
            "iteration : 834 Current training loss = 0.5132936545773845\n",
            "iteration : 835 Current training loss = 0.513068205179311\n",
            "iteration : 836 Current training loss = 0.5128431297788462\n",
            "iteration : 837 Current training loss = 0.512618427289415\n",
            "iteration : 838 Current training loss = 0.5123940966290815\n",
            "iteration : 839 Current training loss = 0.512170136720522\n",
            "iteration : 840 Current training loss = 0.5119465464909965\n",
            "iteration : 841 Current training loss = 0.5117233248723216\n",
            "iteration : 842 Current training loss = 0.5115004708008416\n",
            "iteration : 843 Current training loss = 0.5112779832174016\n",
            "iteration : 844 Current training loss = 0.5110558610673198\n",
            "iteration : 845 Current training loss = 0.5108341033003587\n",
            "iteration : 846 Current training loss = 0.5106127088706988\n",
            "iteration : 847 Current training loss = 0.5103916767369094\n",
            "iteration : 848 Current training loss = 0.5101710058619223\n",
            "iteration : 849 Current training loss = 0.5099506952130044\n",
            "iteration : 850 Current training loss = 0.5097307437617277\n",
            "iteration : 851 Current training loss = 0.5095111504839457\n",
            "iteration : 852 Current training loss = 0.509291914359763\n",
            "iteration : 853 Current training loss = 0.5090730343735088\n",
            "iteration : 854 Current training loss = 0.5088545095137114\n",
            "iteration : 855 Current training loss = 0.5086363387730677\n",
            "iteration : 856 Current training loss = 0.5084185211484192\n",
            "iteration : 857 Current training loss = 0.5082010556407235\n",
            "iteration : 858 Current training loss = 0.507983941255028\n",
            "iteration : 859 Current training loss = 0.5077671770004423\n",
            "iteration : 860 Current training loss = 0.5075507618901128\n",
            "iteration : 861 Current training loss = 0.5073346949411952\n",
            "iteration : 862 Current training loss = 0.5071189751748282\n",
            "iteration : 863 Current training loss = 0.5069036016161071\n",
            "iteration : 864 Current training loss = 0.506688573294058\n",
            "iteration : 865 Current training loss = 0.5064738892416115\n",
            "iteration : 866 Current training loss = 0.506259548495576\n",
            "iteration : 867 Current training loss = 0.5060455500966138\n",
            "iteration : 868 Current training loss = 0.5058318930892131\n",
            "iteration : 869 Current training loss = 0.5056185765216636\n",
            "iteration : 870 Current training loss = 0.5054055994460318\n",
            "iteration : 871 Current training loss = 0.5051929609181348\n",
            "iteration : 872 Current training loss = 0.5049806599975147\n",
            "iteration : 873 Current training loss = 0.5047686957474151\n",
            "iteration : 874 Current training loss = 0.5045570672347562\n",
            "iteration : 875 Current training loss = 0.5043457735301088\n",
            "iteration : 876 Current training loss = 0.5041348137076717\n",
            "iteration : 877 Current training loss = 0.5039241868452461\n",
            "iteration : 878 Current training loss = 0.5037138920242122\n",
            "iteration : 879 Current training loss = 0.5035039283295064\n",
            "iteration : 880 Current training loss = 0.5032942948495953\n",
            "iteration : 881 Current training loss = 0.503084990676454\n",
            "iteration : 882 Current training loss = 0.5028760149055429\n",
            "iteration : 883 Current training loss = 0.502667366635784\n",
            "iteration : 884 Current training loss = 0.5024590449695381\n",
            "iteration : 885 Current training loss = 0.502251049012583\n",
            "iteration : 886 Current training loss = 0.5020433778740897\n",
            "iteration : 887 Current training loss = 0.5018360306666019\n",
            "iteration : 888 Current training loss = 0.5016290065060125\n",
            "iteration : 889 Current training loss = 0.5014223045115435\n",
            "iteration : 890 Current training loss = 0.501215923805723\n",
            "iteration : 891 Current training loss = 0.5010098635143647\n",
            "iteration : 892 Current training loss = 0.5008041227665463\n",
            "iteration : 893 Current training loss = 0.5005987006945903\n",
            "iteration : 894 Current training loss = 0.5003935964340409\n",
            "iteration : 895 Current training loss = 0.5001888091236466\n",
            "iteration : 896 Current training loss = 0.499984337905337\n",
            "iteration : 897 Current training loss = 0.4997801819242052\n",
            "iteration : 898 Current training loss = 0.4995763403284884\n",
            "iteration : 899 Current training loss = 0.4993728122695469\n",
            "iteration : 900 Current training loss = 0.49916959690184587\n",
            "iteration : 901 Current training loss = 0.4989666933829368\n",
            "iteration : 902 Current training loss = 0.49876410087343864\n",
            "iteration : 903 Current training loss = 0.49856181853701964\n",
            "iteration : 904 Current training loss = 0.4983598455403785\n",
            "iteration : 905 Current training loss = 0.49815818105322746\n",
            "iteration : 906 Current training loss = 0.4979568242482756\n",
            "iteration : 907 Current training loss = 0.4977557743012083\n",
            "iteration : 908 Current training loss = 0.4975550303906736\n",
            "iteration : 909 Current training loss = 0.4973545916982641\n",
            "iteration : 910 Current training loss = 0.49715445740849984\n",
            "iteration : 911 Current training loss = 0.49695462670881285\n",
            "iteration : 912 Current training loss = 0.49675509878953106\n",
            "iteration : 913 Current training loss = 0.4965558728438619\n",
            "iteration : 914 Current training loss = 0.4963569480678779\n",
            "iteration : 915 Current training loss = 0.49615832366049967\n",
            "iteration : 916 Current training loss = 0.4959599988234829\n",
            "iteration : 917 Current training loss = 0.4957619727614028\n",
            "iteration : 918 Current training loss = 0.49556424468163895\n",
            "iteration : 919 Current training loss = 0.49536681379436187\n",
            "iteration : 920 Current training loss = 0.49516967931251876\n",
            "iteration : 921 Current training loss = 0.4949728404518197\n",
            "iteration : 922 Current training loss = 0.4947762964307245\n",
            "iteration : 923 Current training loss = 0.49458004647042925\n",
            "iteration : 924 Current training loss = 0.49438408979485304\n",
            "iteration : 925 Current training loss = 0.4941884256306255\n",
            "iteration : 926 Current training loss = 0.493993053207075\n",
            "iteration : 927 Current training loss = 0.4937979717562139\n",
            "iteration : 928 Current training loss = 0.49360318051273033\n",
            "iteration : 929 Current training loss = 0.49340867871397337\n",
            "iteration : 930 Current training loss = 0.49321446559994203\n",
            "iteration : 931 Current training loss = 0.4930205404132753\n",
            "iteration : 932 Current training loss = 0.4928269023992393\n",
            "iteration : 933 Current training loss = 0.4926335508057175\n",
            "iteration : 934 Current training loss = 0.4924404848832005\n",
            "iteration : 935 Current training loss = 0.4922477038847742\n",
            "iteration : 936 Current training loss = 0.49205520706611006\n",
            "iteration : 937 Current training loss = 0.4918629936854557\n",
            "iteration : 938 Current training loss = 0.4916710630036258\n",
            "iteration : 939 Current training loss = 0.49147941428398967\n",
            "iteration : 940 Current training loss = 0.491288046792465\n",
            "iteration : 941 Current training loss = 0.4910969597975078\n",
            "iteration : 942 Current training loss = 0.49090615257010206\n",
            "iteration : 943 Current training loss = 0.49071562438375305\n",
            "iteration : 944 Current training loss = 0.4905253745144771\n",
            "iteration : 945 Current training loss = 0.4903354022407948\n",
            "iteration : 946 Current training loss = 0.490145706843721\n",
            "iteration : 947 Current training loss = 0.4899562876067592\n",
            "iteration : 948 Current training loss = 0.4897671438158907\n",
            "iteration : 949 Current training loss = 0.48957827475956983\n",
            "iteration : 950 Current training loss = 0.4893896797287155\n",
            "iteration : 951 Current training loss = 0.48920135801670306\n",
            "iteration : 952 Current training loss = 0.4890133089193584\n",
            "iteration : 953 Current training loss = 0.4888255317349497\n",
            "iteration : 954 Current training loss = 0.48863802576418286\n",
            "iteration : 955 Current training loss = 0.48845079031019234\n",
            "iteration : 956 Current training loss = 0.48826382467853674\n",
            "iteration : 957 Current training loss = 0.48807712817719046\n",
            "iteration : 958 Current training loss = 0.48789070011653957\n",
            "iteration : 959 Current training loss = 0.4877045398093748\n",
            "iteration : 960 Current training loss = 0.48751864657088523\n",
            "iteration : 961 Current training loss = 0.4873330197186535\n",
            "iteration : 962 Current training loss = 0.4871476585726492\n",
            "iteration : 963 Current training loss = 0.48696256245522435\n",
            "iteration : 964 Current training loss = 0.4867777306911073\n",
            "iteration : 965 Current training loss = 0.48659316260739793\n",
            "iteration : 966 Current training loss = 0.48640885753356156\n",
            "iteration : 967 Current training loss = 0.48622481480142576\n",
            "iteration : 968 Current training loss = 0.4860410337451735\n",
            "iteration : 969 Current training loss = 0.4858575137013389\n",
            "iteration : 970 Current training loss = 0.4856742540088037\n",
            "iteration : 971 Current training loss = 0.4854912540087908\n",
            "iteration : 972 Current training loss = 0.4853085130448613\n",
            "iteration : 973 Current training loss = 0.48512603046290836\n",
            "iteration : 974 Current training loss = 0.4849438056111546\n",
            "iteration : 975 Current training loss = 0.48476183784014676\n",
            "iteration : 976 Current training loss = 0.4845801265027521\n",
            "iteration : 977 Current training loss = 0.4843986709541531\n",
            "iteration : 978 Current training loss = 0.48421747055184516\n",
            "iteration : 979 Current training loss = 0.48403652465563074\n",
            "iteration : 980 Current training loss = 0.4838558326276169\n",
            "iteration : 981 Current training loss = 0.4836753938322108\n",
            "iteration : 982 Current training loss = 0.48349520763611586\n",
            "iteration : 983 Current training loss = 0.4833152734083275\n",
            "iteration : 984 Current training loss = 0.4831355905201305\n",
            "iteration : 985 Current training loss = 0.48295615834509414\n",
            "iteration : 986 Current training loss = 0.48277697625907007\n",
            "iteration : 987 Current training loss = 0.4825980436401865\n",
            "iteration : 988 Current training loss = 0.4824193598688466\n",
            "iteration : 989 Current training loss = 0.4822409243277243\n",
            "iteration : 990 Current training loss = 0.48206273640175995\n",
            "iteration : 991 Current training loss = 0.48188479547815816\n",
            "iteration : 992 Current training loss = 0.48170710094638236\n",
            "iteration : 993 Current training loss = 0.4815296521981541\n",
            "iteration : 994 Current training loss = 0.48135244862744714\n",
            "iteration : 995 Current training loss = 0.4811754896304851\n",
            "iteration : 996 Current training loss = 0.4809987746057372\n",
            "iteration : 997 Current training loss = 0.4808223029539165\n",
            "iteration : 998 Current training loss = 0.48064607407797383\n",
            "iteration : 999 Current training loss = 0.4804700873830968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmY1uNr3dPbk",
        "colab_type": "text"
      },
      "source": [
        "## Test model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG9D6oDB7vQJ",
        "colab_type": "text"
      },
      "source": [
        "#### Get the desired label base on the highest probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R-ebcXO70Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(output):\n",
        "  temp1 = (np.argmax(output, axis=0)) # get the largest element along col\n",
        "  temp3 = np.eye(10)[temp1] # create one_hot vector\n",
        "  return temp3.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OABxwiEdoAW",
        "colab_type": "code",
        "outputId": "d1091376-2f83-4bed-b935-245f423a1b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result = predict(a2)\n",
        "corr = 0\n",
        "for i in range(m_train):\n",
        "  if np.array_equal(result[:,i],y_train[:,i]):\n",
        "    corr = corr + 1\n",
        "print(\"Correctness in training set\",corr/m_train)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correctness Percentage  0.8539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXySH6iQCtTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(input):\n",
        "  z1 = np.dot(w1,input) + b1\n",
        "  a1 = sigmoid(z1)\n",
        "  z2 = np.dot(w2,a1) + b2\n",
        "  a2 = softmax(z2)\n",
        "  return a2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFe4kheCukv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = forward(x_test)\n",
        "prediction = predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72FZ6gkoC2f1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29910755-adb1-4f0e-bf91-8c2816620b98"
      },
      "source": [
        "corr = 0\n",
        "for i in range(m_test):\n",
        "  if np.array_equal(prediction[:,i],y_test[:,i]):\n",
        "    corr = corr + 1\n",
        "print(\"Correctness in test set\",corr/m_test)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correctness in test set 0.8568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZfeBgXreIU8",
        "colab_type": "text"
      },
      "source": [
        "## input Example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXag2ZmPeMg2",
        "colab_type": "code",
        "outputId": "0a2fa179-dde2-48a8-a3e1-e6c9a26b78b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "sample_idx = 111\n",
        "\n",
        "image = x_test[:,sample_idx].reshape((28,28))\n",
        "\n",
        "plt.imshow(image)\n",
        "\n",
        "print(\"NN output = \", test[:,sample_idx])\n",
        "print(\"Model output = \", prediction[:,sample_idx])\n",
        "\n",
        "print(\"The input character = \", y_test[:,sample_idx])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN output =  [1.38412896e-05 8.46216180e-02 3.03463373e-03 7.73707606e-02\n",
            " 1.49982270e-03 5.35822184e-03 3.43227104e-06 7.23519316e-01\n",
            " 1.84116310e-02 8.61667227e-02]\n",
            "Model output =  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "The input character =  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWqbmi065U_W",
        "colab_type": "text"
      },
      "source": [
        "# Try upload your own image and test the NN\n",
        "\n",
        "### you can create the image in the follwing websit: \n",
        "\n",
        "http://kleki.com\n",
        "\n",
        "Please use a larger brush or the storke will disappear after resize. (around 75)\n",
        "\n",
        "The follwing code will automatically resize the image and fed it into the network for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK0sUgt55ac4",
        "colab_type": "code",
        "outputId": "77355220-ab71-45ec-c7d1-f8a2c719e98b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-893b58d0-760c-4f9a-999e-d4d111189609\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-893b58d0-760c-4f9a-999e-d4d111189609\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 2019_12_08_Kleki (1).png to 2019_12_08_Kleki (1).png\n",
            "User uploaded file \"2019_12_08_Kleki (1).png\" with length 34765 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u3vmaQH6rCn",
        "colab_type": "code",
        "outputId": "89c73417-f313-4b75-c712-9637169f21e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "import cv2\n",
        "\n",
        "def inverse_color(image):\n",
        "\n",
        "    height,width = image.shape\n",
        "    img2 = image.copy()\n",
        "\n",
        "    for i in range(height):\n",
        "        for j in range(width):\n",
        "            img2[i,j] = (255-image[i,j]) \n",
        "    return img2\n",
        "\n",
        "img = cv2.imread(fn)\n",
        "resized_img = cv2.resize(img, (28, 28))\n",
        "gray = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
        "gray = inverse_color(gray)\n",
        "\n",
        "plt.imshow(gray)\n",
        "plt.show()\n",
        "\n",
        "normalised_img = gray / 255\n",
        "input_img = normalised_img.reshape((28*28,1))\n",
        "\n",
        "predict_user = forward(input_img)\n",
        "\n",
        "print(\"Model output = \", predict(predict_user))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model output =  [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}